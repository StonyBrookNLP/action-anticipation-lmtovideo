train.train_one_epoch_fn.loss_wts.feat=1.0
train.train_one_epoch_fn.loss_wts.past_cls_action=0.1

train.batch_size=64
eval.batch_size=64
train.num_epochs=15

model/backbone=identity
model.backbone_dim=1024
model/temporal_aggregator=identity
model/future_predictor=avth
model.dropout=0.8
+model.future_predictor.n_head=4
+model.future_predictor.n_layer=2
+model.future_predictor.output_len=1
+model.future_predictor.inter_dim=2048
+model.future_predictor.return_past_too=true
+model.future_predictor.future_pred_loss={_target_: torch.nn.MSELoss}
+model.future_predictor.future_pred_loss_wt=1.0
+model.future_predictor.avg_last_n=1
model.classifier_on_past=true


opt.lr_wd=[[__all__,0.001,0.000001]]
opt.bias_bn_wd_scale=1.0
opt.optimizer.nesterov=true

data_train.num_frames=10
data_train.frame_rate=1
data_train.subclips.num_frames=1
data_train.subclips.stride=1
data_eval=${data_train}

opt/optimizer=sgd
opt/scheduler=cosine

dataset@dataset_train=egtea/anticipation_train
dataset@dataset_eval=egtea/anticipation_val
+dataset@dataset_eval_train=egtea/anticipation_train
dataset_train.sample_strategy=last_clip
dataset_eval.sample_strategy=last_clip
dataset_eval_train.sample_strategy=last_clip
dataset_train.conv_to_anticipate_fn.tau_a=0.5
dataset_train.conv_to_anticipate_fn.tau_o=10
dataset_eval.conv_to_anticipate_fn.tau_a=0.5
dataset_eval.conv_to_anticipate_fn.tau_o=10
dataset_eval_train.conv_to_anticipate_fn.tau_a=0.5
dataset_eval_train.conv_to_anticipate_fn.tau_o=10
dataset.egtea.common.label_type=action
dataset.egtea.common.split=1
dataset.egtea.common.modality=rgb

+dataset_train.conv_to_anticipate_fn.drop_style=correct
+dataset_eval.conv_to_anticipate_fn.drop_style=correct
+dataset_eval_train.conv_to_anticipate_fn.drop_style=correct

hydra.launcher.nodes=1
hydra.launcher.gpus_per_node=2
