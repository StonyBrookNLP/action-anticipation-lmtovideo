train.train_one_epoch_fn.loss_wts.feat=2.0

train.batch_size=32
eval.batch_size=32
train.num_epochs=20

model/backbone=identity
model.backbone_dim=1024
model/temporal_aggregator=identity
model/future_predictor=avth
model.dropout=0.8
+model.future_predictor.n_head=8
+model.future_predictor.n_layer=12
+model.future_predictor.output_len=1
+model.future_predictor.inter_dim=2048
+model.future_predictor.return_past_too=true
+model.future_predictor.future_pred_loss={_target_: torch.nn.MSELoss}
+model.future_predictor.future_pred_loss_wt=1.0
+model.future_predictor.avg_last_n=1


opt.lr_wd=[[__all__,0.000005,0.0001]]
opt.bias_bn_wd_scale=1.0

data_train.num_frames=10
data_train.frame_rate=2
data_eval.num_frames=${data_train.num_frames}
data_eval.frame_rate=${data_train.frame_rate}

opt/optimizer=adam
opt/scheduler=cosine
opt.warmup.num_epochs=5
opt.scheduler.num_epochs=15  # total - 5 warmup

dataset@dataset_train=epic_kitchens/anticipation_train_minus_val
dataset@dataset_eval=epic_kitchens/anticipation_val
dataset_train.sample_strategy=last_clip
dataset_eval.sample_strategy=last_clip
dataset_train.conv_to_anticipate_fn.tau_o=5
dataset_eval.conv_to_anticipate_fn.tau_o=5
dataset.epic_kitchens.common.label_type=action
+dataset_train.reader_fn={_target_: datasets.epic_kitchens.EpicRULSTMFeatsReader, lmdb_path: ${dataset.epic_kitchens.common.rulstm_feats_dir}/rgb/, read_type: normal}
+dataset_eval.reader_fn=${dataset_train.reader_fn}

+dataset_train.conv_to_anticipate_fn.drop_style=correct
+dataset_eval.conv_to_anticipate_fn.drop_style=correct

hydra.launcher.nodes=1
hydra.launcher.gpus_per_node=4
